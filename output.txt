The MLP architecture is as follows. The input nodes connect to 10 hidden layer nodes which then connect to the 8 output nodes through weights.
Initial weights
First Layer: [[ 0.29294876  0.05221891 -0.05254989 -0.33829909 -0.08180986 -0.00348106
  -0.00668913 -0.27888874  0.02901399 -0.28477886]
 [-0.4888513  -0.62121106  0.18377292  0.29689108 -0.19021931  0.30242161
  -0.32864048  0.09230924 -0.11828735 -0.50811891]
 [-0.09382016  0.09381664 -0.02692243  0.13009649 -0.1161315  -0.39433376
   0.07074483 -0.3832456  -0.53208328  0.22076168]
 [-0.20483639  0.21797015  0.05911507 -0.17295201 -0.25195753 -0.08066414
  -0.20156496 -0.1717903   0.34801564 -0.45865621]
 [-0.29257249  0.24948543  0.45142261  0.02606508  0.09880822  0.32860382
  -0.20679012  0.41917168 -0.34526525 -0.03553893]
 [-0.01943146  0.13931418  0.17664741  0.23532198 -0.75438992  0.02328132
   0.50774376 -0.26976136  0.1136008  -0.00698626]
 [-0.04009546 -0.40299534 -0.16952617  0.19511636 -0.04278603 -0.12651316
  -0.910328   -0.00332728  0.1752921  -0.32857254]
 [-0.41689417 -0.27272383 -0.1508823  -0.6911904  -0.18349438  0.19048214
   0.0836954  -0.43011904  0.06766492  0.36355191]
 [-0.89324518  0.687222   -0.3626332  -0.30207133  0.36106375  0.03267624
   0.39008351  0.07604304 -0.256146    0.14188778]
 [ 0.07912001 -0.75578548  0.09191355 -0.41914783 -0.34574773  0.2883653
   0.08033186  0.25965054 -0.01919522 -0.02311111]]
Second Layer: [[-0.26990325 -0.25991888 -0.02165423  0.24038812  0.03915406  0.77936399
   0.43951861  0.25929256 -0.24030444 -0.05040167]
 [-0.54616854  0.09267065  0.47563038 -0.21045085  0.67069468 -0.27542388
  -0.43487414 -0.40810899  0.17983573  0.37041785]
 [-0.04357773  0.20521489 -0.0383292  -0.01568788 -0.24981712  0.37757504
  -0.68132757 -0.17565604  0.35513971 -0.34637457]
 [-0.01303866  0.22702354  0.31522378  0.00975036 -0.05440317 -0.28347743
   0.3682558   0.13877142  0.50483252  0.62000514]
 [-0.29801707 -0.13774934 -0.24676804  0.05061692  0.24722054  0.12005645
   0.16343806  0.16940784  0.12859086 -0.2431338 ]
 [ 0.08085004  0.37222752  0.28719803 -0.10888865  0.19347014 -0.04815025
  -0.00245583  0.40937164  0.08861102  0.34355755]
 [ 0.03144863 -0.23456313 -0.08532051  0.12446321 -0.13823009  0.10794517
   0.75212682  0.19797093 -0.30383373 -0.53802013]
 [ 0.20476534  0.01008604 -0.29398678 -0.02971077  0.02128084 -0.17501357
   0.41387251  0.16275245  0.42082867 -0.17470564]]
Calculating epoch...
Epochs required: 110958
Second Layer part has ended
Final weights
First Layer: [[ -9.86226644  16.711846    11.190491   -13.39559252   1.15492291
  -31.27686996  -4.26376944  -6.98574391  19.15292569   7.21127344]
 [ -6.66641369   6.27193875   1.44403149   2.55292278  -1.66705219
    9.5160378    2.64375117  -2.78292997  -9.99271199  -1.71348056]
 [ 37.18093505  -5.4455515    3.32707484   3.59604779   2.98357571
  -17.90732086 -18.39508798 -13.79476007 -14.24184138  -1.13849281]
 [ -6.47753798   7.30329133   1.58348621  -8.76113164  -0.07460663
   12.36415093  -1.47646312  -5.19165207   1.07110152 -16.41917097]
 [ -4.63133289   5.12475529  13.72355755   9.53756204  -1.33419654
  -10.6219815   -4.69830051   1.11167463   0.1073384    3.02236019]
 [  1.56185363 -11.32322271 -17.5686862  -13.51485241   2.65387371
   15.99814448   9.75410527   2.1865667   -0.40328714  -3.20841303]
 [  3.20724568  -2.24570298  -3.53842164 -13.31528106   5.50717975
   -0.3192433   -3.04858185  -1.99111195  10.28365165  -0.16662802]
 [  2.61952214 -12.40852555   7.34652732 -15.65633546   1.08049064
   -0.81906952 -10.21143197 -15.38630253   8.42139984  22.42626249]
 [ -3.2376401   24.9705582   -4.39388308  -5.10719193   6.08482732
   -1.63239603 -16.4544929    6.02673546 -18.38596408  -5.02339788]
 [  5.27253479  -7.67109927   0.69001176  -1.2050411   -0.77913209
   -6.05650145   2.38859078   3.31250366  11.53914815  -4.9596935 ]]
Second Layer: [[-0.63582877 -0.95324563 -0.17511805  1.43979931 -0.83770509  0.15767272
  -0.65349384 -0.09527377  0.47598414  0.21250014]
 [-2.80863004 -7.00563429  0.54098247  5.63453313  8.6786985   2.76788324
   0.46944773 -0.67071232 -3.14633069 -7.5702531 ]
 [ 1.49044637 -1.23893093  1.93224503  0.08837798  0.17980188  2.60272849
  -0.59809855 -1.70517422 -0.61875933 -2.9227471 ]
 [ 1.17800258  6.7190577   0.85627784 -3.09403091 -9.19724918 -5.72741607
  -1.55473426  2.07927769  1.03834882  6.6588344 ]
 [-3.34382413 -1.67061227 -3.08139789  0.99354261 -0.46227314 -3.45362435
  -0.10065026  3.25802647  3.63037257  2.05339106]
 [ 0.81210634 -6.25336339 -2.97425369  5.0473044   4.31369549  4.01765413
  -4.15150747  1.51366249  0.53339213 -3.68780034]
 [ 0.49468099  0.40470618  0.23927175 -2.20158181 -0.82929011 -1.89834518
   4.20347114 -3.71635092 -1.14489669 -0.9743412 ]
 [-0.16933654  0.05310276 -0.2037632  -0.52656845 -0.70813353 -0.81718675
   0.51446076  0.21122908 -0.58814305 -0.69670294]]
Holdout Accuracy: 0.4
Print matrix and metrics
class 1
True Positive: 1
False Positive: 2
True Negative: 17
False Negative: 0
Accuracy: 0.9
Error Rate: 0.1
Precision: 0.1
Recall: 0.1
class 2
True Positive: 2
False Positive: 1
True Negative: 17
False Negative: 0
Accuracy: 0.95
Error Rate: 0.05
Precision: 0.05
Recall: 0.05
class 3
True Positive: 1
False Positive: 4
True Negative: 15
False Negative: 0
Accuracy: 0.8
Error Rate: 0.2
Precision: 0.2
Recall: 0.2
class 4
True Positive: 3
False Positive: 6
True Negative: 11
False Negative: 0
Accuracy: 0.7
Error Rate: 0.3
Precision: 0.3
Recall: 0.3
class 5
True Positive: 1
False Positive: 5
True Negative: 14
False Negative: 0
Accuracy: 0.75
Error Rate: 0.25
Precision: 0.25
Recall: 0.25
class 6
True Positive: 0
False Positive: 2
True Negative: 18
False Negative: 0
Accuracy: 0.9
Error Rate: 0.1
Precision: 0.1
Recall: 0.1
class 7
True Positive: 0
False Positive: 4
True Negative: 16
False Negative: 0
Accuracy: 0.8
Error Rate: 0.2
Precision: 0.2
Recall: 0.2
class 8
True Positive: 0
False Positive: 0
True Negative: 20
False Negative: 0
Accuracy: 1.0
Error Rate: 0.0
Precision: 0.0
Recall: 0.0
None